train:
  batch_size: 256
  num_workers: 3
  max_epochs: 5
  optimizer: adamw
  lr: 0.00008
  weight_decay: 0.0001
  gradient_accumulation_steps: 2
  freeze_backbone: -3