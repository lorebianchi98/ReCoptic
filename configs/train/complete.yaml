train:
  batch_size: 64
  num_workers: 4
  max_epochs: 20
  optimizer: adamw
  lr: 0.0001
  weight_decay: 0.0001
  gradient_accumulation_steps: 2
  freeze_backbone: False