train:
  batch_size: 256
  num_workers: 3
  max_epochs: 3
  optimizer: adamw
  lr: 0.00001
  ltype: infonce
  weight_decay: 0.0001
  gradient_accumulation_steps: 1
  freeze_backbone: -3