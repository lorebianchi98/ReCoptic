{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import re\n",
    "import shutil\n",
    "import json\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Folder renaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'VatLib.Borg.Copt'\n",
    "to_replace = 'Borg.copt.'\n",
    "def rename_files_and_dirs(base_path):\n",
    "    # Walk through all directories and files in the base path\n",
    "    for root, dirs, files in os.walk(base_path, topdown=False):\n",
    "        # Rename files\n",
    "        for file_name in files:\n",
    "            if to_replace in file_name:\n",
    "                new_name = file_name.replace(to_replace, '')\n",
    "                old_file = os.path.join(root, file_name)\n",
    "                new_file = os.path.join(root, new_name)\n",
    "                os.rename(old_file, new_file)\n",
    "                print(f'Renamed file: {old_file} -> {new_file}')\n",
    "\n",
    "        # Rename directories (we must do this after files, so we walk bottom-up)\n",
    "        for dir_name in dirs:\n",
    "            if to_replace in dir_name:\n",
    "                new_name = dir_name.replace(to_replace, '')\n",
    "                old_dir = os.path.join(root, dir_name)\n",
    "                new_dir = os.path.join(root, new_name)\n",
    "                os.rename(old_dir, new_dir)\n",
    "                print(f'Renamed directory: {old_dir} -> {new_dir}')\n",
    "                \n",
    "rename_files_and_dirs(base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset explorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matching_strs = [\"{:05d}{}\".format(i, side) for i in range(1000) for side in ['r', 'v']]\n",
    "pattern = re.compile(r\"[0-9]\\d{2}[rv]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_keep_dir = 'coptic_dataset'\n",
    "to_delete_dir = 'trash'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(to_keep_dir, exist_ok=True)\n",
    "os.makedirs(to_delete_dir, exist_ok=True)\n",
    "\n",
    "to_keep = []\n",
    "to_delete = []\n",
    "\n",
    "for collection in os.listdir(base_path):\n",
    "    collection_path = os.path.join(base_path, collection)\n",
    "\n",
    "    if not os.path.isdir(collection_path):\n",
    "        continue  # Skip non-directory entries\n",
    "\n",
    "    for coll_path, _, files in os.walk(collection_path):\n",
    "        for file in files:\n",
    "            src_file = os.path.join(coll_path, file)\n",
    "\n",
    "            # Compute the relative path to maintain folder structure\n",
    "            relative_path = os.path.relpath(coll_path, base_path)\n",
    "\n",
    "            if pattern.search(file):\n",
    "                dest_dir = os.path.join(to_keep_dir, relative_path)\n",
    "                to_keep.append(src_file)\n",
    "            else:\n",
    "                dest_dir = os.path.join(to_delete_dir, relative_path)\n",
    "                to_delete.append(src_file)\n",
    "\n",
    "            # Create the directory only if there is at least one file\n",
    "            os.makedirs(dest_dir, exist_ok=True)\n",
    "            shutil.copy2(src_file, os.path.join(dest_dir, file))\n",
    "\n",
    "print(f\"Copied {len(to_keep)} files to 'to_keep' directory.\")\n",
    "print(f\"Copied {len(to_delete)} files to 'to_delete' directory.\")\n",
    "\n",
    "len(to_keep), len(to_delete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6677"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_keep_dir = '../coptic_dataset'\n",
    "collections = os.listdir(to_keep_dir)\n",
    "n_collections = len(collections)\n",
    "n_collections\n",
    "\n",
    "len_x_collection = {collection: len(os.listdir(os.path.join(to_keep_dir, collection))) for collection in collections}\n",
    "n_images = sum(len_x_collection.values())\n",
    "df = pd.DataFrame(list(len_x_collection.items()), columns=['Document', 'N images'])\n",
    "# df.to_csv(\"collection_occurrences.csv\", index=False)\n",
    "\n",
    "n_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4006, 667, 2004, 6677)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_dataset(n_images, split_percentages):\n",
    "    indices = list(range(n_images))\n",
    "    random.shuffle(indices)\n",
    "\n",
    "    train_size = int(n_images * split_percentages['train'])\n",
    "    val_size = int(n_images * split_percentages['val'])\n",
    "\n",
    "    train_ids = indices[:train_size]\n",
    "    val_ids = indices[train_size:train_size + val_size]\n",
    "    test_ids = indices[train_size + val_size:]\n",
    "\n",
    "    return train_ids, val_ids, test_ids\n",
    "\n",
    "split_percentages = {\n",
    "    'train': 0.6,\n",
    "    'val': 0.1,\n",
    "    'test': 0.3,\n",
    "}\n",
    "\n",
    "# Ensure reproducibility (optional)\n",
    "random.seed(42)\n",
    "\n",
    "train_ids, val_ids, test_ids = split_dataset(n_images, split_percentages)\n",
    "len(train_ids), len(val_ids), len(test_ids), len(set(train_ids + val_ids + test_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_path = 'annotations/coptic_dataset.json'\n",
    "image_id = 0\n",
    "\n",
    "data = {\n",
    "    'images': []\n",
    "}\n",
    "\n",
    "for collection in os.listdir(to_keep_dir):\n",
    "    collection_path = os.path.join(to_keep_dir, collection)\n",
    "    \n",
    "    if not os.path.isdir(collection_path):\n",
    "        continue  # Skip non-directory entries\n",
    "\n",
    "    for coll_path, _, files in os.walk(collection_path):\n",
    "        for file in files:\n",
    "            if image_id in train_ids:\n",
    "                split = 'train'\n",
    "            elif image_id in val_ids:\n",
    "                split = 'val'\n",
    "            elif image_id in test_ids:\n",
    "                split = 'test'\n",
    "            else:\n",
    "                raise (\"Error\")\n",
    "            \n",
    "            data['images'].append(\n",
    "                {\n",
    "                    'id': image_id,\n",
    "                    'filename': file,\n",
    "                    'collection': collection,\n",
    "                    'filepath': os.path.join(collection, file),\n",
    "                    'split': split\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            image_id += 1\n",
    "            \n",
    "\n",
    "with open(annotation_path, 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_path = 'annotations/coptic_dataset.json'\n",
    "with open(annotation_path, 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def find_recto_verso_pairs_single_collection(filenames):\n",
    "    page_map = {}\n",
    "\n",
    "    for i, name in enumerate(filenames):\n",
    "        match = re.search(r'_(\\d{4})([rv])(?=[_.])', name)\n",
    "        if match:\n",
    "            page_num, side = match.groups()\n",
    "            # key = name[:match.span()[1] - 1] + '_' + name[match.span()[1]:]\n",
    "            key = name[match.span()[0] - 3:match.span()[1] - 1]\n",
    "            if key not in page_map:\n",
    "                page_map[key] = {}\n",
    "            page_map[key][side] = name\n",
    "\n",
    "    pairs = []\n",
    "    for page_num, sides in page_map.items():\n",
    "        if 'r' in sides and 'v' in sides:\n",
    "            pairs.append((sides['r'], sides['v']))\n",
    "\n",
    "    return pairs\n",
    "\n",
    "def find_recto_verso_pairs(data):\n",
    "    coll_filenames = {}\n",
    "    for img in data['images']:\n",
    "        coll_filenames[img['collection']] = coll_filenames.setdefault(img['collection'], []) + [img['filename']]\n",
    "\n",
    "    coll_filenames = {k: [s for s in v] for k, v in coll_filenames.items()}\n",
    "    pairs = [x for f in coll_filenames.values() for x in find_recto_verso_pairs_single_collection(f)]\n",
    "\n",
    "    # Create a lookup dictionary for fast index retrieval\n",
    "    filename_to_index = {img['filename']: idx for idx, img in enumerate(data['images'])}\n",
    "\n",
    "    # Convert pairs to index pairs\n",
    "    index_pairs = [(filename_to_index[a], filename_to_index[b]) for a, b in pairs]\n",
    "    \n",
    "    return index_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****Train****\n",
      "N. images: 6677\n",
      "N. positives: 592694\n",
      "N. negatives: 7429321\n",
      "N. Total 8022015\n",
      "****Val****\n",
      "N. images: 6677\n",
      "N. positives: 220540\n",
      "N. negatives: 2673363\n",
      "N. Total 2893903\n",
      "****Test****\n",
      "N. images: 6677\n",
      "N. positives: 855307\n",
      "N. negatives: 10515575\n",
      "N. Total 11370882\n"
     ]
    }
   ],
   "source": [
    "def get_statistics(data, split=None, include_train_couples=False, include_val_couples=False, verbose=True):\n",
    "    splits = [split] if type(split) is not list else split\n",
    "    if include_train_couples:\n",
    "        splits_supp = ['train']\n",
    "        if include_val_couples:\n",
    "            splits_supp += ['val']\n",
    "        train_positive_couples, train_negative_couples, data = get_statistics(data, split=splits_supp, include_train_couples=False, verbose=False)\n",
    "        splits += splits_supp\n",
    "    # split_data['images'] = [elem for elem in data['images'] if split is None or elem['split'] in splits]\n",
    "\n",
    "    categories = [elem['collection'] for elem in data['images']]\n",
    "    splits_list = [elem['split'] for elem in data['images']]\n",
    "\n",
    "    # Generate all pairs (positive and negative)\n",
    "    positive_pairs = [(i, j) for i in range(len(categories)) for j in range(len(categories)) if i < j and categories[i] == categories[j] and (splits_list[i] in splits and splits_list[j] in splits)]\n",
    "    negative_pairs = [(i, j) for i in range(len(categories)) for j in range(len(categories)) if i < j and categories[i] != categories[j] and (splits_list[i] in splits and splits_list[j] in splits)]\n",
    "\n",
    "    if include_train_couples:\n",
    "        positive_pairs = list(set(positive_pairs) - set(train_positive_couples))\n",
    "        negative_pairs = list(set(negative_pairs) - set(train_negative_couples))\n",
    "    \n",
    "        # deleting recto and verso couples\n",
    "        positive_pairs = list(set(positive_pairs) - set(find_recto_verso_pairs(data)))\n",
    "    len_x_collection = {}\n",
    "    for img in data['images']:\n",
    "        len_x_collection[img['collection']] = len_x_collection.get(img['collection'], 0) + 1\n",
    "\n",
    "    n_images = sum(len_x_collection.values())\n",
    "    df = pd.DataFrame(list(len_x_collection.items()), columns=['Document', 'N images'])\n",
    "\n",
    "    # print(df)\n",
    "    if verbose:\n",
    "        print(f\"N. images: {n_images}\")\n",
    "        print(f\"N. positives: {len(positive_pairs)}\")\n",
    "        print(f\"N. negatives: {len(negative_pairs)}\")\n",
    "        print(f\"N. Total {len(positive_pairs) + len(negative_pairs)}\")\n",
    "    else:\n",
    "        return positive_pairs, negative_pairs, data\n",
    "    \n",
    "\n",
    "print(\"****Train****\")\n",
    "get_statistics(data, 'train', include_train_couples=False)\n",
    "print(\"****Val****\")\n",
    "get_statistics(data, 'val', include_train_couples=True, include_val_couples=False)\n",
    "print(\"****Test****\")\n",
    "get_statistics(data, 'test', include_train_couples=True, include_val_couples=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****Total****\n",
      "N. positives 1668541\n",
      "N. negatives 20618259\n"
     ]
    }
   ],
   "source": [
    "total_poss = []\n",
    "total_negs = []\n",
    "for split in ['train', 'val', 'test']:\n",
    "    poss, negs, _ = get_statistics(data, split, include_train_couples=split != 'train', include_val_couples=split == 'test', verbose=False)\n",
    "    total_poss += poss\n",
    "    total_negs += negs\n",
    "print(\"****Total****\")\n",
    "print(f\"N. positives {len(total_poss)}\")\n",
    "print(f\"N. negatives {len(total_negs)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coptic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
